# multiagent_ddpg: MADDPG Implementation for Multi-Agent Environments

This project implements the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm for cooperative multi-agent environments, as part of a Reinforcement Learning course (TME8).

## Overview

The goal of this assignment is to implement and evaluate the MADDPG algorithm, which extends the DDPG algorithm to handle the challenges of multi-agent reinforcement learning, such as non-stationarity and the need for coordination.

## Files

*   `multiagent_ddpg.py`: The main Python script containing the MADDPG implementation, including the actor and critic networks, replay memory, training loop, and environment setup.

## Environment Setup

1.  **Install Dependencies:**
    ```bash
    pip install gym matplotlib numpy torch
    pip install git+https://github.com/openai/multiagent-particle-envs.git
    ```
    Note:  You might need to install `box2d-py` separately if you encounter issues with the multiagent-particle-envs installation.  Try `pip install box2d-py`.

## Algorithms Implemented

### Multi-Agent Deep Deterministic Policy Gradient (MADDPG)

*   An extension of the DDPG algorithm to handle multi-agent environments.
*   Uses a centralized critic that has access to the observations and actions of all agents, allowing it to learn a more accurate Q-function.
*   Uses decentralized actors, where each agent learns its own policy based only on its own observations.
*   Addresses the non-stationarity problem in multi-agent environments by training the critic on a dataset of experiences generated by all agents.

## Usage

1.  Run the script:
    ```bash
    python multiagent_ddpg.py
    ```

2.  The script will:
    *   Create the `simple_spread` multi-agent environment (or another environment from the `multiagent-particle-envs` package).
    *   Train the MADDPG agent.
    *   Generate a plot of the episode rewards over time, displaying the learning progress.

## Code Structure

*   **`make_env` Function:** Creates the multi-agent environment using the `multiagent-particle-envs` package.
*   **`CriticNet` Class:** Implements the critic network for estimating the Q-value function, taking the observations and actions of all agents as input.
    *   `__init__(self, dim_observation, dim_action)`: Initializes the Critic network.
    *   `forward(self, obs, acts)`: Performs a forward pass through the network to compute the Q-value for a given state-action pair.
*   **`ActorNet` Class:** Implements the actor network for generating actions for each agent, taking only the agent's own observation as input.
    *   `__init__(self, dim_observation, dim_action)`: Initializes the Actor network.
    *   `forward(self, obs)`: Performs a forward pass through the network to compute the action for a given state.
*   **`Experience` Namedtuple:** Represents a single experience (states, actions, next\_states, rewards, dones) for storing replay data.
*   **`ReplayMemory` Class:** Implements a replay memory buffer for storing and sampling past experiences.
    *   `__init__(self, capacity)`: Initializes the replay memory with a specified capacity.
    *   `push(self, *args)`: Saves a transition to memory.
    *   `sample(self, batch_size)`: Samples a random batch of transitions from memory.
    *   `__len__(self)`: Returns the number of transitions currently stored in memory.
*   **`MADDPG_Agent` Class:** Implements the MADDPG agent.
    *   `__init__(self, env, batch_size, replay_capacity, episodes_before_train, device='cpu')`: Initializes the MADDPG agent.
    *   `select_actions(self, actor_nets, states, noise=True)`: Selects actions for each agent based on their individual policies.
    *   `learn(self, batch)`: Updates the actor and critic networks for all agents based on a sampled batch of experiences.
    *   `soft_update(self, target, source, t)`: Softly updates the parameters of the target network using the parameters of the source network.
    *   `train(self, n_episodes, max_episode_length=100)`: Trains the MADDPG agent.

## Key Parameters

*   `batch_size`: The number of transitions sampled from the replay memory for each training update.
*   `replay_capacity`: The capacity of the replay memory buffer.
*   `episodes_before_train`: The number of episodes to run before starting training.
*   `device`: The device to run the training on ('cpu' or 'cuda').

## Discussion and Further Exploration

*   **Environments:** Test the MADDPG implementation on different multi-agent environments from the `multiagent-particle-envs` package, such as `simple_adversary` or `simple_tag`.
*   **Hyperparameter Tuning:** Experiment with different values of the hyperparameters, such as the learning rates, hidden size, discount factor, soft update coefficient, and exploration noise, to observe their effect on the learning process.
*   **Network Architectures:** Investigate different network architectures for the actor and critic networks, such as adding more hidden layers, using different activation functions, or incorporating attention mechanisms.
*   **Communication:** Explore the possibility of adding communication channels between agents to improve coordination.
*   **Centralized vs. Decentralized Critics:** Compare the performance of MADDPG with a centralized critic to a version with decentralized critics.
*   **Bonus Tasks:** Implement the bonus tasks mentioned in the assignment description, such as using multiple policies per agent to improve robustness.

## Notes

*   The code uses the PyTorch framework for implementing the neural networks. Make sure you have PyTorch installed.
*   The multi-agent environments are from the `multiagent-particle-envs` package. Make sure you have installed it using the instructions above.
*   This implementation provides a basic MADDPG solution for cooperative multi-agent environments. You can further improve the performance by tuning the hyperparameters, using more advanced network architectures, or implementing other MADDPG extensions.
